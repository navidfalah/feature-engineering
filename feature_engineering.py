# -*- coding: utf-8 -*-
"""feature_engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8_xRvMNg4LS4HM4RwffLKRX1dR6roMS
"""

! pip install openml

import openml

adult = openml.datasets.get_dataset(1590)

X, y, _, _ = adult.get_data(target=adult.default_target_attribute)

X.shape, y.shape

X.head()

## check all of the values for the redundant naming

for col in X.columns:
  print(f'{col}: {X[col].unique()}')



gender_counts = X['sex'].value_counts()
print(gender_counts)

import pandas as pd

print("original features :\n", list(X.columns), "\n")
data_dummies = pd.get_dummies(X)
print("new features :\n", list(data_dummies.columns))

data_dummies.head()

data_dummies.shape

features = data_dummies.loc[:, 'age':'occupation_Transport-moving']

X = features.values
data_dummies.columns

y

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
print(model.score(X_test, y_test))

demo_df = pd.DataFrame({"integer feature": [0, 1, 2, 1],
                        "categorical feature": ['socks', 'fox', 'socks', 'box']})
demo_df

pd.get_dummies(demo_df)

demo_df['integer feature'] = demo_df['integer feature'].astype(str)
pd.get_dummies(demo_df, columns=['integer feature', 'categorical feature'])

! pip install mglearn

import mglearn
import numpy as np
import matplotlib.pyplot as plt

np.linspace(-3, 3, 1000, endpoint=False)

np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor

X, y = mglearn.datasets.make_wave(n_samples=100)
line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)

reg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)
plt.plot(line, reg.predict(line), label="Decision Tree")

reg = LinearRegression().fit(X, y)
plt.plot(line, reg.predict(line), label="Logistic Regression")

plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend()

bins = np.linspace(-3, 3, 11)
bins

which_bin = np.digitize(X, bins=bins)
print(X[:5])
print(which_bin[:5])

from sklearn.preprocessing import OneHotEncoder

# Remove the sparse argument, or set sparse=True if using a newer scikit-learn version.
encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse
encoder.fit(which_bin)
x_binned = encoder.transform(which_bin)
print(x_binned[:5])

line_binned = encoder.transform(np.digitize(line, bins=bins))
print(line_binned[:5])

reg = LinearRegression().fit(x_binned, y)
plt.plot(line, reg.predict(line_binned), label="Logistic Regression binned")

reg = DecisionTreeRegressor(min_samples_split=3).fit(x_binned, y)
plt.plot(line, reg.predict(line_binned), label="Decision Tree binned")

plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend()
plt.vlines(bins, -3, 3, linestyles='--')
plt.plot(X[:, 0], y, 'o', c='k')

x_combined = np.hstack([x_binned, X])
print(x_combined[:5])

reg = LinearRegression().fit(x_combined, y)
line_combined = np.hstack([line_binned, line])
plt.plot(line, reg.predict(line_combined), label="Logistic Regression combined")

for bin in bins:
    plt.plot([bin, bin], [-3, 3], ':', c='k')

plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend()
plt.plot(X[:, 0], y, 'o', c='k')

X_product = np.hstack([x_binned, X**x_binned])
print(X_product[:5])
print(X_product.shape)

reg = LinearRegression().fit(X_product, y)

linear_product = np.hstack([line_binned, line**line_binned])
plt.plot(line, reg.predict(linear_product), label="Logistic Regression product")

for bin in bins:
    plt.plot([bin, bin], [-3, 3], ':', c='k')

plt.plot(X[:, 0], y, 'o', c="k")
plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend()

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=10, include_bias=False)
poly.fit(X)
x_poly = poly.transform(X)

x_poly.shape

print("polynominal feature names: {}".format(poly.get_feature_names_out()))

reg = LinearRegression().fit(x_poly, y)

line_poly = poly.transform(line)
plt.plot(line, reg.predict(line_poly), label="Logistic Regression product")

plt.plot(X[:, 0], y, 'o', c="k")
plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend()

from sklearn.svm import SVR

for gamma in [1, 10]:
  svr = SVR(kernel='rbf', gamma=gamma, C=10).fit(X, y)
  plt.plot(line, svr.predict(line), label="gamma={}".format(gamma))

plt.plot(X[:, 0], y, 'o', c="k")
plt.ylabel("Probability")
plt.xlabel("Feature")
plt.legend(loc="best")

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

boston = fetch_california_housing()
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train_scaled)
X_poly_test = poly.transform(X_test_scaled)

print("X_poly_train.shape: {}".format(X_poly_train.shape))
print("X_poly_test.shape: {}".format(X_poly_test.shape))
print(poly.get_feature_names_out())

from sklearn.linear_model import Ridge

ridge = Ridge().fit(X_train_scaled, y_train)
print("Training score: {:.2f}".format(ridge.score(X_test_scaled, y_test)))

ridge = Ridge().fit(X_poly_train, y_train)
print("Training score: {:.2f}".format(ridge.score(X_poly_test, y_test)))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=0)
rf.fit(X_train_scaled, y_train)

print("Training set score: {:.2f}".format(rf.score(X_train_scaled, y_train)))
print("Test set score: {:.2f}".format(rf.score(X_test_scaled, y_test)))

rf = RandomForestRegressor(n_estimators=100, random_state=0)
rf.fit(X_poly_train, y_train)

print("Training set score: {:.2f}".format(rf.score(X_poly_train, y_train)))
print("Test set score: {:.2f}".format(rf.score(X_poly_test, y_test)))

rnd = np.random.RandomState(0)
X_org = rnd.normal(size=(1000, 3))
w = rnd.normal(size=3)

X = np.random.poisson(10 * np.exp(X_org))
y = np.dot(X_org, w)

print(np.bincount(X[:, 0]))

X.shape

bins = np.bincount(X[:, 0])
plt.bar(range(len(bins)), bins)
plt.ylabel("Number of appearances")
plt.xlabel("Value")

from sklearn.linear_model import Ridge

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
score = Ridge().fit(X_train, y_train).score(X_test, y_test)
print("Test score: {:.3f}".format(score))

x_train_log = np.log(X_train + 1)
x_test_log = np.log(X_test + 1)

score = Ridge().fit(x_train_log, y_train).score(x_test_log, y_test)
print("Test score: {:.3f}".format(score))

plt.hist(np.log(X_train[:, 0] + 1), bins=25, color="gray")
plt.ylabel("Number of appearances")
plt.xlabel("Value")

#### automatic feature selection

from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import SelectPercentile
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()

rng = np.random.RandomState(42)
noise = rng.normal(size=(len(cancer.data), 50))

print(cancer.data.shape)

X_w_noise = np.hstack([cancer.data, noise])
X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=0, test_size=.5)

select = SelectPercentile(percentile=50)
select.fit(X_train, y_train)
X_train_selected = select.transform(X_train)
print("X_train.shape: {}".format(X_train.shape))
print("X_train_selected.shape: {}".format(X_train_selected.shape))

mask = select.get_support()
print(mask)
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")

from sklearn.linear_model import LogisticRegression

X_test_selected = select.transform(X_test)
lr = LogisticRegression(max_iter=5000).fit(X_train, y_train)

print("Score with all features: {:.3f}".format(lr.score(X_test, y_test)))

lr = LogisticRegression(max_iter=5000).fit(X_train_selected, y_train)
print("Score with only selected features: {:.3f}".format(lr.score(X_test_selected, y_test)))

### model based feature selection

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold="median")
select.fit(X_train, y_train)

X_train_l1 = select.transform(X_train)
print("X_train.shape: {}".format(X_train.shape))
print("X_train_l1.shape: {}".format(X_train_l1.shape))

mask = select.get_support()
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")

X_test_l1 = select.transform(X_test)
lr = LogisticRegression(max_iter=5000).fit(X_train_l1, y_train).score(X_test_l1, y_test)
print("Test score: {:.3f}".format(lr))

from sklearn.feature_selection import RFE

select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)
select.fit(X_train, y_train)

mask = select.get_support()
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")

X_train_rfe = select.transform(X_train)
X_test_rfe = select.transform(X_test)

lr = LogisticRegression(max_iter=5000).fit(X_train_rfe, y_train)
print("Test score: {:.3f}".format(lr.score(X_test_rfe, y_test)))

### use the model used inside of rfe to predict
select.score(X_test, y_test), select.score(X_train, y_train)

citibike = mglearn.datasets.load_citibike()

print("citi bike data {}".format(citibike.head()))

plt.figure(figsize=(10, 3))
xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),
 freq='D')
plt.xticks(xticks, xticks.strftime("%a %m-%d"), rotation=90, ha="left")
plt.plot(citibike, linewidth=1)
plt.xlabel("Date")
plt.ylabel("Rentals")

y = citibike.values

X = citibike.index.strftime("%s").astype(int).values.reshape(-1, 1)

n_train = 184

def eval_on_features(features, target, regressor):
  X_train, X_test = features[:n_train], features[n_train:]
  y_train, y_test = target[:n_train], target[n_train:]
  regressor.fit(X_train, y_train)
  print("test set r2 : {:.2f}".format(regressor.score(X_test, y_test)))
  y_pred = regressor.predict(X_test)
  y_pred_train = regressor.predict(X_train)
  plt.figure(figsize=(10, 3))
  plt.xticks(range(0, len(X), 8), xticks.strftime("%a %m-%d"), rotation=90, ha="left")
  plt.plot(range(n_train), y_train, label="train")
  plt.plot(range(n_train, len(y_test) + n_train), y_test, '-', label="test")
  plt.plot(range(n_train), y_pred_train, '--', label="prediction train")
  plt.plot(range(n_train, len(y_test) + n_train), y_pred, '--',
  label="prediction test")
  plt.legend(loc=(1.01, 0))
  plt.xlabel("Date")
  plt.ylabel("Rentals")

from sklearn.ensemble import RandomForestRegressor

regressor = RandomForestRegressor(n_estimators=100, random_state=0)
plt.figure()
eval_on_features(X, y, regressor)

X_hour = citibike.index.hour.values.reshape(-1, 1)
eval_on_features(X_hour, y, regressor)

X_hour_week = np.hstack([X_hour, citibike.index.dayofweek.values.reshape(-1, 1)])
eval_on_features(X_hour_week, y, regressor)

from sklearn.linear_model import LinearRegression

regressor = LinearRegression()
plt.figure()
eval_on_features(X_hour_week, y, regressor)

enc = OneHotEncoder()
X_hour_week_onehot = enc.fit_transform(X_hour_week)
print(X_hour_week_onehot.shape)
print(X_hour_week.shape)

eval_on_features(X_hour_week_onehot, y, regressor)

poly_transformer = PolynomialFeatures(degree=2, interaction_only=True,
 include_bias=False)
X_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)
lr = Ridge()
eval_on_features(X_hour_week_onehot_poly, y, lr)

hour = ["%02d:00" % i for i in range(0, 24, 3)]
day = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
features = day + hour

features_poly = poly_transformer.get_feature_names_out(features)
features_nonzero = np.array(features_poly)[lr.coef_ != 0]
coef_nonzero = lr.coef_[lr.coef_ != 0]

plt.figure(figsize=(15, 2))
plt.plot(coef_nonzero, 'o')
plt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)
plt.xlabel("Feature magnitude")
plt.ylabel("Feature")

